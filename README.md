Gemini-RAG â€” Smart Document Q&A System (Production-Ready RAG)
ğŸš€ Project Goal

Build a production-ready Retrieval-Augmented Generation (RAG) system using Google Gemini + Vector Database, where users can upload documents and ask natural-language questions.

âœ… Answers are strictly grounded in uploaded documents
âŒ No hallucinated or out-of-context responses

This project mirrors real-world AI Engineer work in SaaS, support tools, and internal knowledge assistants.


ğŸ¯ Key Features

| Feature                             | Status |
| ----------------------------------- | ------ |
| Upload multiple PDF / TXT documents | âœ…      |
| Automatic chunking with overlap     | âœ…      |
| Generate embeddings using Gemini    | âœ…      |
| Store vectors in Pinecone           | âœ…      |
| Semantic similarity search          | âœ…      |
| Context-aware answers with sources  | âœ…      |
| Chat-style frontend UI              | âœ…      |
| Environment-based config (prod/dev) | âœ…      |
| Fully deployed (Frontend + Backend) | âœ…      |


ğŸ§  Why RAG?

Large Language Models alone hallucinate because they donâ€™t know your private data.

Retrieval-Augmented Generation (RAG) solves this by:

âœ” Fetching relevant document context first
âœ” Injecting retrieved chunks into the LLM prompt
âœ” Producing grounded, explainable answers
âœ” Working with private & enterprise data
âœ” Scaling for support bots, internal search & AI copilots

If no relevant context is found â†’
The system responds: â€œNot available in the document.â€
(Interviewers love this ğŸ”¥)


ğŸ— High-Level Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User      â”‚
â”‚  (Frontend)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Query
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (Node.js) â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  1. Embed Query    â”‚
â”‚  2. Vector Search  â”‚
â”‚  3. Build Context  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Top-K Chunks
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pinecone Vector  â”‚
â”‚      Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Retrieved Context
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gemini LLM       â”‚
â”‚ (Context + Prompt) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Answer + Sources
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User Output     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ“‚ Folder Structure

Gemini-RAG/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ indexer.js
â”‚   â”‚   â”œâ”€â”€ pineconeClient.js
â”‚   â”‚   â””â”€â”€ ragService.js
â”‚   â”œâ”€â”€ gemini.js
â”‚   â”œâ”€â”€ uploads/
â”‚   â”œâ”€â”€ server.js
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ .env.sample
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatWindow.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUploader.jsx
â”‚   â”‚   â”‚   â””â”€â”€ MessageBubble.jsx
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ .env.sample
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ README.md


ğŸ§© Tech Stack

| Component  | Technology                |
| ---------- | ------------------------- |
| Frontend   | React + Vite              |
| Backend    | Node.js + Express         |
| LLM        | Google Gemini             |
| Embeddings | Gemini Embeddings         |
| Vector DB  | Pinecone                  |
| RAG        | Custom implementation     |
| Deployment | Vercel (FE) + Render (BE) |


ğŸ” Production Considerations

âœ” Environment-based configs (.env)
âœ” API keys never exposed to frontend
âœ” Rate-limit ready architecture
âœ” Modular RAG pipeline (easy to extend)

Live demo is shared on request to manage LLM API usage.


ğŸ§ª Testing Strategy

âœ” Ask known questions from uploaded docs
âœ” Verify retrieved chunks before generation
âœ” Validate answer relevance
âœ” Handle empty retrieval gracefully


ğŸ›  Setup & Run (Local)
Backend
cd backend
npm install
npm start

Frontend
cd frontend
npm install
npm run dev

Create .env files:

backend

GEMINI_API_KEY=your_key
PINECONE_API_KEY=your_key
PINECONE_INDEX=your_index


frontend

VITE_API_BASE_URL=http://localhost:5000


ğŸ“Œ Portfolio Note

This project is part of my transition from
Senior MERN Engineer â†’ AI / GenAI Engineer

Future improvements:

Authentication & multi-user support
Metadata filtering
Retrieval evaluation metrics
Advanced chunking strategies
