# Gemini-RAG â€” Smart FAQ Bot (Retrieval-Augmented Generation)

## ğŸš€ Project Goal

Build a **Retrieval-Augmented Generation (RAG)** system using **Google Gemini + ChromaDB**, where users can upload documents and ask questions.
The answers are **grounded in actual uploaded content**, minimizing hallucinations.

This aligns with **real-world AI Engineer skills** required in SaaS/Support products.

---

## ğŸ¯ Key Features

| Feature                          | Status |
| -------------------------------  | ------ |
| Load multipledocuments (PDF/Text)| âœ”      |
| Chunk documents into segments    | âœ”      |
| Generate Embeddings (Gemini)     | âœ”      |
| Store vectors in ChromaDB        | âœ”      |
| Similarity Search on User Query  | âœ”      |
| Final grounded answer via LLM    | âœ”      |

---

## ğŸ§  Why RAG?

Large Language Models alone hallucinate because they donâ€™t know your private data.
Retrieval-Augmented Generation (RAG) solves this by:

* Provides **up-to-date information**
* Injecting **retrieved chunks into the LLM prompt**
* Reduces **hallucinations**
* Works with **private company data**
* Scales for **enterprise search + AI support tools**

---

## ğŸ— High-Level Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Frontend   â”‚
â”‚  (React/Vite) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ REST API
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend     â”‚
â”‚  (Express)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAG Pipeline      â”‚
â”‚  - Chunking        â”‚
â”‚  - Embeddings      â”‚
â”‚  - Pinecone Search â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemini LLM        â”‚
â”‚  Context-Aware     â”‚
â”‚  Answer Generation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Folder Structure

```text
Gemini-RAG/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ indexer.js        # Chunking + overlap strategy
â”‚   â”‚   â”œâ”€â”€ pineconeClient.js # Pinecone config
â”‚   â”‚   â””â”€â”€ ragService.js     # Indexing & query logic
â”‚   â”œâ”€â”€ gemini.js             # Gemini API config
â”‚   â”œâ”€â”€ server.js             # Express server
â”‚   â”œâ”€â”€ uploads/              # Uploaded files
â”‚   â”œâ”€â”€ .env                  # Environment variables
â”‚   â”œâ”€â”€ .env.sample           # Sample env file
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatWindow.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUploader.jsx
â”‚   â”‚   â”‚   â””â”€â”€ MessageBubble.jsx
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ .env                  
â”‚   â”œâ”€â”€ .env.sample
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§© Tech Stack

| Component       | Tool                     |
| --------------- | -------------------------|
| LLM             | Gemini                   |
| Vector Database | Pinecone                 |
| Backend         | Node.js + Express        |
| Embeddings      | Gemini Embeddings        |
| Frontend        | React + Vite             |
| Language        | JavaScript (ES Modules)  |
| Deployment      | Vercel (FE) + Render (BE)|

---

## ğŸ§ª How It Works (RAG Flow)

1. **User uploads PDF/Text files**.
2. Files are **chunked** into small segments.
3. **Gemini embeddings** generated per chunk.
4. Chunks stored in **Pinecone vector database**.
5. User submits **query**.
6. Backend performs **similarity search** in vector DB.
7. Top-K chunks passed to **Gemini LLM**.
8. **Grounded answer** returned to user.

> If no context is found â†’ Responds: â€œ**Not available in document.**â€

***Live demo is shared on request to manage LLM API usage.**

---

## âš™ï¸ Setup & Run

### Backend

```bash
cd backend
npm install
npm start
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

## Create .env files:

### Backend

```bash
GEMINI_API_KEY=your_key
PINECONE_API_KEY=your_key
PINECONE_INDEX=your_index
```

### Frontend

```bash
VITE_API_BASE_URL=http://localhost:5000
```

---

## ğŸ”® Future Improvements

* Add **multi-user authentication**
* Enhance **UI/UX** (loader, error handling)
* Add **analytics for queries**
* Extend to **AI Agents / Vision** integrations
* Deploy with **monitoring & logging**

---
